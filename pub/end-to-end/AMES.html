<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <!-- Syntax highlighting via Prism, note: restricted langs -->
<link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/judoc.css">
  <link rel="stylesheet" href="/css/pure.css">
  <link rel="stylesheet" href="/css/side-menu.css">
  <link rel="stylesheet" href="/css/extra.css">
  <link rel="icon" href="/assets/infra/favicon.ico">
   <title></title>  
</head>
<body>
  <div id="layout">
    <!-- Menu toggle / hamburger icon -->
    <a href="#menu" id="menuLink" class="menu-link"><span></span></a>
    <div id="menu">
      <div class="pure-menu">
        <a href="/" id="menu-logo-link">
          <div class="menu-logo">
            <img id="menu-logo" alt="MLJ Logo" src="/assets/infra/MLJLogo2.svg" />
            <p><strong>MLJ Tutorials</strong></p>
          </div>
        </a>
        <ul class="pure-menu-list">
          <li class="pure-menu-item pure-menu-top-item "><a href="/" class="pure-menu-link"><strong>Home</strong></a></li>

          <li class="pure-menu-sublist-title"><strong>Getting started</strong></li>
          <ul class="pure-menu-sublist">
            <li class="pure-menu-item "><a href="/pub/getting-started/choosing-a-model.html" class="pure-menu-link">⊳ Choosing a model</a></li>
            <li class="pure-menu-item "><a href="/pub/getting-started/fit-and-predict.html" class="pure-menu-link">⊳ Fit, predict, transform</a></li>
            <li class="pure-menu-item "><a href="/pub/getting-started/model-tuning.html" class="pure-menu-link">⊳ Model tuning</a></li>
            <li class="pure-menu-item "><a href="/pub/getting-started/composing-models.html" class="pure-menu-link">⊳ Composing models</a></li>
            <li class="pure-menu-item "><a href="/pub/getting-started/learning-networks.html" class="pure-menu-link">⊳ Learning networks</a></li>
          </ul>

          <li class="pure-menu-sublist-title"><strong>End to end examples</strong></li>
          <ul class="pure-menu-sublist">
            <li class="pure-menu-item pure-menu-selected"><a href="/pub/end-to-end/AMES.html" class="pure-menu-link">⊳ AMES</a></li>
          </ul>
        </ul>
      </div>
    </div>
    <div id="main"> <!-- Closed in foot -->
      

<!-- Content appended here -->

<div class="jd-content">
<h1 id="ames"><a href="/pub/end-to-end/AMES.html#ames">AMES</a></h1>
<em>Download the</em> <a href="https://raw.githubusercontent.com/tlienart/MLJTutorials/gh-pages/notebooks/EX-AMES.ipynb" target="_blank"><em>notebook</em></a> <em>or the</em> <a href="https://raw.githubusercontent.com/tlienart/MLJTutorials/gh-pages/scripts/EX-AMES.jl" target="_blank"><em>raw script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> </p>
<div class="jd-toc"><ol><li><a href="/pub/end-to-end/AMES.html#ames">AMES</a><ol><li><a href="/pub/end-to-end/AMES.html#baby_steps">Baby steps</a></li><li><a href="/pub/end-to-end/AMES.html#dummy_model">Dummy model</a></li><li><a href="/pub/end-to-end/AMES.html#knn-ridge_blend">KNN-Ridge blend</a><ol><li><a href="/pub/end-to-end/AMES.html#using_the_expanded_syntax">Using the expanded syntax</a></li><li><a href="/pub/end-to-end/AMES.html#using_the_pipeline_syntax">Using the pipeline syntax</a></li><li><a href="/pub/end-to-end/AMES.html#tuning_the_model">Tuning the model</a></li></ol></li></ol></li></ol></div>
<h2 id="baby_steps"><a href="/pub/end-to-end/AMES.html#baby_steps">Baby steps</a></h2>
<p>Let&#39;s load a reduce version of the well-known Ames House Price data set &#40;containing six of the more important categorical features and six of the more important numerical features&#41;. As &quot;iris&quot; the dataset is so common that you can load it directly with <code>@load_ames</code> and the reduce version via <code>@load_reduced_ames</code>
<pre><code class="language-julia">using MLJ, MLJBase, PrettyPrinting, DataFrames, Statistics

X, y = @load_reduced_ames
@show size(X)
first(X, 3) |> pretty</code></pre><div class="code_output"><pre><code class="plaintext">size(X) = (1456, 12)
┌─────────────────────────────────────────────────┬────────────────────────────┬─────────────────────────────────────────────┬────────────────────────────┬────────────────────────────┬────────────────────────────┬────────────────────────────┬───────────────────────┬─────────────────────────────────────────────┬───────────────────────┬────────────────────────────┬────────────────────────────┐
│ OverallQual                                     │ GrLivArea                  │ Neighborhood                                │ x1stFlrSF                  │ TotalBsmtSF                │ BsmtFinSF1                 │ LotArea                    │ GarageCars            │ MSSubClass                                  │ GarageArea            │ YearRemodAdd               │ YearBuilt                  │
│ CategoricalArrays.CategoricalValue{Int64,UInt8} │ Float64                    │ CategoricalArrays.CategoricalString{UInt32} │ Float64                    │ Float64                    │ Float64                    │ Float64                    │ Int64                 │ CategoricalArrays.CategoricalString{UInt32} │ Int64                 │ Float64                    │ Float64                    │
│ ScientificTypes.OrderedFactor{10}               │ ScientificTypes.Continuous │ ScientificTypes.Multiclass{25}              │ ScientificTypes.Continuous │ ScientificTypes.Continuous │ ScientificTypes.Continuous │ ScientificTypes.Continuous │ ScientificTypes.Count │ ScientificTypes.Multiclass{15}              │ ScientificTypes.Count │ ScientificTypes.Continuous │ ScientificTypes.Continuous │
├─────────────────────────────────────────────────┼────────────────────────────┼─────────────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼───────────────────────┼─────────────────────────────────────────────┼───────────────────────┼────────────────────────────┼────────────────────────────┤
│ 5                                               │ 816.0                      │ Mitchel                                     │ 816.0                      │ 816.0                      │ 816.0                      │ 6600.0                     │ 2                     │ _20                                         │ 816                   │ 2003.0                     │ 1982.0                     │
│ 8                                               │ 2028.0                     │ Timber                                      │ 2028.0                     │ 1868.0                     │ 1460.0                     │ 11443.0                    │ 3                     │ _20                                         │ 880                   │ 2006.0                     │ 2005.0                     │
│ 7                                               │ 1509.0                     │ Gilbert                                     │ 807.0                      │ 783.0                      │ 0.0                        │ 7875.0                     │ 2                     │ _60                                         │ 393                   │ 2003.0                     │ 2003.0                     │
└─────────────────────────────────────────────────┴────────────────────────────┴─────────────────────────────────────────────┴────────────────────────────┴────────────────────────────┴────────────────────────────┴────────────────────────────┴───────────────────────┴─────────────────────────────────────────────┴───────────────────────┴────────────────────────────┴────────────────────────────┘
</code></pre></div>
<p>and the target is a continuous vector:</p>
<pre><code class="language-julia">@show y[1:3]
scitype_union(y)</code></pre><div class="code_output"><pre><code class="plaintext">y[1:3] = [138000.0, 369900.0, 180000.0]
ScientificTypes.Continuous</code></pre></div>
<p>so this is a standard regression problem with a mix of categorical and continuous input.</p>
<h2 id="dummy_model"><a href="/pub/end-to-end/AMES.html#dummy_model">Dummy model</a></h2>
<p>A model is just a container for hyperparameters, let&#39;s take a particularly simple one: the constant regression.</p>
<pre><code class="language-julia">creg = ConstantRegressor()</code></pre><div class="code_output"><pre><code class="plaintext">[34mConstantRegressor @ 4…88[39m</code></pre></div>
<p>Wrapping the model in data creates a <em>machine</em> which will store training outcomes &#40;<em>fit-results</em>&#41;</p>
<pre><code class="language-julia">cmach = machine(creg, X, y)</code></pre><div class="code_output"><pre><code class="plaintext">[34mMachine{ConstantRegressor} @ 7…63[39m</code></pre></div>
<p>You can now train the machine specifying the data it should be trained on &#40;if unspecified, all the data will be used&#41;;</p>
<pre><code class="language-julia">train, test = partition(eachindex(y), 0.70, shuffle=true); # 70:30 split
fit!(cmach, rows=train)
ŷ = predict(cmach, rows=test)
ŷ[1:3]</code></pre><div class="code_output"><pre><code class="plaintext">Distributions.Normal{Float64}[Distributions.Normal{Float64}(μ=181022.65848871443, σ=74975.21062525548), Distributions.Normal{Float64}(μ=181022.65848871443, σ=74975.21062525548), Distributions.Normal{Float64}(μ=181022.65848871443, σ=74975.21062525548)]</code></pre></div>
<p>Observe that the output is probabilistic, each element is a univariate normal distribution &#40;with the same mean and variance as it&#39;s a constant model...&#41;.</p>
<p>You can recover deterministic output by either computing the mean of predictions or using <code>predict_mean</code> directly:</p>
<pre><code class="language-julia">ŷ = predict_mean(cmach, rows=test)
ŷ[1:3]</code></pre><div class="code_output"><pre><code class="plaintext">[181022.65848871443, 181022.65848871443, 181022.65848871443]</code></pre></div>
<p>You can then call one of the loss functions to assess the quality of the model by comparing the performances on the test set:</p>
<pre><code class="language-julia">rmsl(ŷ, y[test])</code></pre><div class="code_output"><pre><code class="plaintext">0.42407113929256157</code></pre></div>
<h2 id="knn-ridge_blend"><a href="/pub/end-to-end/AMES.html#knn-ridge_blend">KNN-Ridge blend</a></h2>
<p>Let&#39;s try something a bit better than a constant regressor.</p>
<ul>
<li><p>one-hot-encode categorical inputs</p>
</li>
<li><p>log-transform the target</p>
</li>
<li><p>fit both a KNN regression and a Ridge regression on the data</p>
</li>
<li><p>Compute a weighted average of individual model predictions</p>
</li>
<li><p>inverse transform &#40;exponentiate&#41; the blended prediction</p>
</li>
</ul>
<p>You will first define a fixed model where all hyperparameters are specified or set to default. Then you will see how to create a model around a learning network that can be tuned.</p>
<pre><code class="language-julia">@load RidgeRegressor pkg="MultivariateStats"
@load KNNRegressor</code></pre><div class="code_output"><pre><code class="plaintext">[34mKNNRegressor @ 7…69[39m</code></pre></div>
<h3 id="using_the_expanded_syntax"><a href="/pub/end-to-end/AMES.html#using_the_expanded_syntax">Using the expanded syntax</a></h3>
<p>Let&#39;s start by defining the source nodes:</p>
<pre><code class="language-julia">Xs, ys = source.((X, y));</code></pre>
<p>On the &quot;first layer&quot;, there&#39;s one hot encoder and a log transform, these will respectively lead to node <code>W</code> and node <code>z</code>:</p>
<pre><code class="language-julia">hot = machine(OneHotEncoder(), Xs)

W = transform(hot, Xs)
z = log(ys);</code></pre>
<p>On the &quot;second layer&quot;, there&#39;s a KNN regressor and a ridge regressor, these lead to node <code>ẑ₁</code> and <code>ẑ₂</code>
<pre><code class="language-julia">knn   = machine(KNNRegressor(K=5), W, z)
ridge = machine(RidgeRegressor(lambda=2.5), W, z)

ẑ₁ = predict(ridge, W)
ẑ₂ = predict(knn, W)</code></pre><div class="code_output"><pre><code class="plaintext">[34mNode{NodalMachine{KNNRegressor}} @ 2…17[39m</code></pre></div>
<p>On the &quot;third layer&quot;, there&#39;s a weighted combination of the two regression models:</p>
<pre><code class="language-julia">ẑ = 0.3ẑ₁ + 0.7ẑ₂;</code></pre>
<p>And finally we need to invert the initial transformation of the target &#40;which was a log&#41;:</p>
<pre><code class="language-julia">ŷ = exp(ẑ);</code></pre>
<p>You&#39;ve now defined a full learning network which you can fit and use for prediction:</p>
<pre><code class="language-julia">fit!(ŷ, rows=train)
ypreds = ŷ(rows=test)
rmsl(y[test], ypreds)</code></pre><div class="code_output"><pre><code class="plaintext">0.18189112690914405</code></pre></div>
<h3 id="using_the_pipeline_syntax"><a href="/pub/end-to-end/AMES.html#using_the_pipeline_syntax">Using the pipeline syntax</a></h3>
<p>If you&#39;re using Julia 1.3, you can use the following syntax to do the same thing.</p>
<p><em>First layer</em>: one hot encoding and log transform:</p>
<pre><code class="language-julia">W = Xs |> OneHotEncoder()
z = ys |> log;</code></pre><div class="code_output"><pre><code class="plaintext">There was an error running the code:
MethodError([34mOneHotEncoder @ 4…91[39m, ([34mSource{:input} @ 5…11[39m,), 0x0000000000006d8b)
</code></pre></div>
<p><em>Second layer</em>: KNN Regression and Ridge regression</p>
<pre><code class="language-julia">ẑ₁ = (W, z) |> KNNRegressor(K=5)
ẑ₂ = (W, z) |> RidgeRegressor(lambda=2.5);</code></pre><div class="code_output"><pre><code class="plaintext">There was an error running the code:
MethodError([34mKNNRegressor @ 1…77[39m, (([34mNode{NodalMachine{OneHotEncoder}} @ 1…51[39m, [34mNode{Nothing} @ 1…53[39m),), 0x0000000000006d8b)
</code></pre></div>
<p><em>Third layer</em>: weighted sum of the two models:</p>
<pre><code class="language-julia">ẑ = 0.3ẑ₁ + 0.7ẑ₂;</code></pre>
<p>then the inverse transform</p>
<pre><code class="language-julia">ŷ = exp(ẑ);</code></pre>
<p>You can then fit and evaluate the model as usual:</p>
<pre><code class="language-julia">fit!(ŷ, rows=train)
rmsl(y[test], ŷ(rows=test))</code></pre><div class="code_output"><pre><code class="plaintext">0.18189112690914405</code></pre></div>
<h3 id="tuning_the_model"><a href="/pub/end-to-end/AMES.html#tuning_the_model">Tuning the model</a></h3>
<p>So far the hyperparameters were explicitly given but it makes more sense to learn them. For this, we define a model around the learning network which can then be trained and tuned as any model:</p>
<pre><code class="language-julia">mutable struct KNNRidgeBlend <: DeterministicNetwork
    knn_model::KNNRegressor
    ridge_model::RidgeRegressor
    knn_weight::Float64
end</code></pre>
<p>We must specify how such a model should be fit, which is effectively just the learning network we had defined before except that now the parameters are contained in the struct:</p>
<pre><code class="language-julia">function MLJ.fit(model::KNNRidgeBlend, X, y)
    Xs, ys = source.((X, y))
    hot = machine(OneHotEncoder(), Xs)
    W = transform(hot, Xs)
    z = log(ys)
    ridge_model = model.ridge_model
    knn_model = model.knn_model
    ridge = machine(ridge_model, W, z)
    knn = machine(knn_model, W, z)
    # and finally
    ẑ = model.knn_weight * predict(knn, W) + (1.0 - model.knn_weight) * predict(ridge, W)
    ŷ = exp(ẑ)
    fit!(ŷ, verbosity=0)
    return ŷ
end</code></pre>
<p><strong>Note</strong>: you really  want to set <code>verbosity&#61;0</code> here otherwise in the tuning you will get a lot of verbose output&#33;</p>
<p>You can now instantiate and fit such a model:</p>
<pre><code class="language-julia">krb = KNNRidgeBlend(KNNRegressor(K=5), RidgeRegressor(lambda=2.5), 0.3)
mach = machine(krb, X, y)
fit!(mach, rows=train)

preds = predict(mach, rows=test)
rmsl(y[test], preds)</code></pre><div class="code_output"><pre><code class="plaintext">0.14154108874199542</code></pre></div>
<p>But more interestingly, the hyperparameters of the model can be tuned.</p>
<p>Before we get started, it&#39;s important to note that the hyperparameters of the model have different levels of <em>nesting</em>. This becomes explicit when trying to access elements:</p>
<pre><code class="language-julia">@show krb.knn_weight
@show krb.knn_model.K
@show krb.ridge_model.lambda</code></pre><div class="code_output"><pre><code class="plaintext">krb.knn_weight = 0.3
krb.knn_model.K = 5
krb.ridge_model.lambda = 2.5
</code></pre></div>
<p>You can also see all the hyperparameters using the <code>params</code> function:</p>
<pre><code class="language-julia">params(krb) |> pprint</code></pre><div class="code_output"><pre><code class="plaintext">(knn_model = (K = 5,
              algorithm = :kdtree,
              metric = Distances.Euclidean(0.0),
              leafsize = 10,
              reorder = true,
              weights = :uniform),
 ridge_model = (lambda = 2.5,),
 knn_weight = 0.3)
</code></pre></div>
<p>The range of values to do your hyperparameter tuning over should follow the nesting structure reflected by <code>params</code>:</p>
<pre><code class="language-julia">k_range = range(krb, :(knn_model.K), lower=2, upper=100, scale=:log10)
l_range = range(krb, :(ridge_model.lambda), lower=1e-4, upper=10, scale=:log10)
w_range = range(krb, :(knn_weight), lower=0.1, upper=0.9)

ranges = [k_range, l_range, w_range]</code></pre><div class="code_output"><pre><code class="plaintext">MLJ.NumericRange{T,Symbol} where T[[34mNumericRange @ 1…12[39m, [34mNumericRange @ 1…45[39m, [34mNumericRange @ 1…78[39m]</code></pre></div>
<p>Now there remains to define how the tuning should be done, let&#39;s just specify a very coarse grid tuning with cross validation and instantiate a tuned model:</p>
<pre><code class="language-julia">tuning = Grid(resolution=3)
resampling = CV(nfolds=6)

tm = TunedModel(model=krb, tuning=tuning, resampling=resampling,
                ranges=ranges, measure=rmsl)</code></pre><div class="code_output"><pre><code class="plaintext">[34mDeterministicTunedModel @ 6…33[39m</code></pre></div>
<p>which we can now finally fit...</p>
<pre><code class="language-julia">mtm = machine(tm, X, y)
fit!(mtm, rows=train);</code></pre>
<p>To retrieve the best model, you can use:</p>
<pre><code class="language-julia">krb_best = fitted_params(mtm).best_model
@show krb_best.knn_model.K
@show krb_best.ridge_model.lambda
@show krb_best.knn_weight</code></pre><div class="code_output"><pre><code class="plaintext">krb_best.knn_model.K = 2
krb_best.ridge_model.lambda = 0.03162277660168379
krb_best.knn_weight = 0.1
</code></pre></div>
<p>you can also use it to make predictions</p>
<pre><code class="language-julia">preds = predict(mtm, rows=test)
rmsl(y[test], preds)</code></pre><div class="code_output"><pre><code class="plaintext">0.12981604581973447</code></pre></div>
<div class="page-foot">
  <div class="copyright">
    &copy; Thibaut Lienart. Last modified: October 15, 2019. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>.
  </div>
</div>

</div>
<!-- CONTENT ENDS HERE -->
      </div> <!-- end of id=main -->
  </div> <!-- end of id=layout -->
  <script src="/libs/pure/ui.min.js"></script>
  
  
      <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

  
</body>
</html>
